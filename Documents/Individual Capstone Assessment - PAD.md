# Individual Capstone Assessment

For our senior design project, my teammate and I wanted to work on something that excites us: robotics and natural language processing (NLP). With our shared interests and complementary skillsets, we aim to tackle a real-world problem by designing an attachment module for a robot similar to Boston Dynamics’ Spot. This module will assist visually impaired individuals by enabling the robot to understand voice commands and announce detected objects via a camera module.

The decision to focus on this project aligns with our enthusiasm for solving practical challenges using advanced technologies. While I’ve participated in hackathons and worked on AI-powered applications, this project feels unique because it combines natural language processing, computer vision, and robotics into a single, modular solution. The attachment’s modularity is key—it allows it to be used with robots beyond the one we’re developing on, as long as they have similar capabilities. This approach ensures the work we’re doing has broader applications and impact.

I don’t see much direct overlap between my coursework and this project, as I’m taking Senior Design a year early, but CS 5134: Natural Language Processing has proven highly relevant. Taught by Professor Tianyu Jiang, the course explored NLP’s evolution from early techniques like n-grams to modern transformer-based models. This deep dive into NLP inspired me to think critically about how to build robust systems that go beyond simple implementations, like using pre-built prompts. My teammate has also taken this course, which will help us collaboratively apply what we’ve learned.

My co-op experiences have also prepared me well for this project. During my first co-op with Dr. Atluri and Cincinnati Children’s Hospital, I gained significant experience in data wrangling, visualization, and machine learning, which sharpened my problem-solving skills in AI. These skills will be invaluable for tasks like processing visual data from the camera module or training models for object detection. My second co-op provided further exposure to managing multiple projects and improving time management and collaboration skills. These soft skills will help keep our project organized and on track. My third co-op focused on web technologies and APIs, which, while not directly related, gave me experience working with backend systems that could prove useful if we need to design flexible data pipelines or modular interfaces.

Hackathons have been another significant source of inspiration. I’ve often worked on AI-powered applications, which taught me how AI can solve problems that traditional software engineering cannot. For example, AI can recognize objects, interpret natural language, or analyze sentiments—tasks impossible without advanced models. This fascination with AI stems from its ability to democratize intelligence and make specialized knowledge accessible to anyone. For this project, I hope to contribute to this democratization by enabling a robot to perform tasks that are otherwise challenging for visually impaired individuals, such as detecting obstacles or identifying objects in their environment.

Our project plan reflects our interdisciplinary goals. Initially, we’ll focus on designing the hardware for the attachment module. Using an NVIDIA Jetson, audio module, camera module, and other equipment, we’ll ensure the attachment is modular and not dependent on the robot’s native sensors (except for universal components like cameras or speakers). From a software perspective, we’ll use ROS 2, which is already installed on the robot, to integrate our systems. The first functionality we’ll implement is voice command processing, leveraging NLP techniques to ensure accurate and flexible command interpretation. Next, we’ll develop an object detection system to identify and announce nearby items via the camera and speaker.

Testing will occur in phases. During the winter break, we’ll focus on hardware and module development, using mock environments to test basic functionalities. Once we regain access to the robot, we’ll conduct real-life field tests to validate the system’s performance in assisting visually impaired individuals. Our ultimate goal is to create an attachment module that works seamlessly and can be deployed on similar robots in various settings. Success will be measured by how well the module performs under real-world conditions and by feedback from testers, particularly those who are visually impaired.

This project represents a significant challenge, but it’s one I’m deeply passionate about. It allows me to combine my skills in natural language processing, computer vision, and engineering to create a solution with tangible impact. By the end, I hope we’ll have a modular system that not only meets the needs of our target users but also inspires further innovation in this space.